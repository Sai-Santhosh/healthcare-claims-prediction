<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/scikit--learn-1.2+-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" />
  <img src="https://img.shields.io/badge/AWS-Integrated-FF9900?style=for-the-badge&logo=amazon-aws&logoColor=white" />
  <img src="https://img.shields.io/badge/Status-Production_Ready-success?style=for-the-badge" />
</p>

<h1 align="center">ğŸ¥ Medical Claims Paid Amount Prediction</h1>

<p align="center">
  <strong>A Production-Grade Machine Learning Pipeline for Healthcare Claims Analytics</strong>
</p>

<p align="center">
  Enterprise-ready ML system processing 17M+ medical claims to predict insurance payment amounts
</p>

---

## ğŸ“‹ Executive Summary

| Metric | Value |
|--------|-------|
| **Dataset Size** | ~17 Million rows, 63 columns |
| **Unique Claims** | ~6.5 Million individual claims |
| **Data Volume** | 3.7 GB raw data |
| **Best Model RÂ²** | 0.44 (Random Forest) |
| **Prediction Target** | Paid Amount per Procedure |

---

## ğŸ¯ Problem Statement

### Business Context
Medical claims processing is a critical function in healthcare insurance. Accurately predicting the **Paid Amount** for medical procedures enables:

- **Cost Estimation**: Predict healthcare costs before procedures
- **Fraud Detection**: Identify anomalous claims
- **Resource Planning**: Better financial forecasting
- **Provider Negotiations**: Data-driven contract discussions

### Dataset Overview
Commercial medical claims filed by healthcare providers in 2016 in New Hampshire:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š DATASET STATISTICS                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Total Records:        16,982,295 rows                          â”‚
â”‚  Total Features:       63 columns                               â”‚
â”‚  Unique Claims:        ~6.5 million                             â”‚
â”‚  NH Residents:         88%                                      â”‚
â”‚  Out-of-State:         12%                                      â”‚
â”‚  File Size:            3.73 GB                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PRODUCTION ML PIPELINE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   AWS S3    â”‚
                              â”‚  Raw Data   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA INGESTION LAYER                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Chunked Loader â”‚  â”‚ Claim Sampler  â”‚  â”‚ Data Validator â”‚                  â”‚
â”‚  â”‚   (100K rows)  â”‚  â”‚  (1M claims)   â”‚  â”‚ (Quality Gates)â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEATURE ENGINEERING LAYER                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Data Cleaner   â”‚  â”‚  Transformer   â”‚  â”‚ Feature Engine â”‚                  â”‚
â”‚  â”‚ â€¢ Missing vals â”‚  â”‚ â€¢ Encoding     â”‚  â”‚ â€¢ Dummies      â”‚                  â”‚
â”‚  â”‚ â€¢ Negatives    â”‚  â”‚ â€¢ Age/Gender   â”‚  â”‚ â€¢ Scaling      â”‚                  â”‚
â”‚  â”‚ â€¢ Duplicates   â”‚  â”‚ â€¢ ICD codes    â”‚  â”‚ â€¢ Log features â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL TRAINING LAYER                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Linear Models  â”‚  â”‚ Ensemble Modelsâ”‚  â”‚ Model Registry â”‚                  â”‚
â”‚  â”‚ â€¢ Lasso        â”‚  â”‚ â€¢ Random Forestâ”‚  â”‚ â€¢ Versioning   â”‚                  â”‚
â”‚  â”‚ â€¢ Ridge        â”‚  â”‚ â€¢ Gradient Bst â”‚  â”‚ â€¢ Metadata     â”‚                  â”‚
â”‚  â”‚ â€¢ ElasticNet   â”‚  â”‚ â€¢ AdaBoost     â”‚  â”‚ â€¢ Deployment   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEPLOYMENT LAYER                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   AWS Lambda   â”‚  â”‚   API Gateway  â”‚  â”‚   S3 Models    â”‚                  â”‚
â”‚  â”‚  Inference API â”‚  â”‚   REST Endpointâ”‚  â”‚  Model Storage â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Model Results & Performance

### Model Comparison

| Model | Validation RÂ² | RMSE | MAE | Training Time |
|-------|---------------|------|-----|---------------|
| **ğŸ† Random Forest** | **0.4368** | $XXX | $XXX | ~5 min |
| MARS (Earth) | 0.2954 | $XXX | $XXX | ~3 min |
| AdaBoost | 0.2274 | $XXX | $XXX | ~8 min |
| Ridge Regression | 0.1351 | $XXX | $XXX | ~15 sec |
| Lasso Regression | 0.1227 | $XXX | $XXX | ~15 sec |

### Best Model Configuration

```yaml
Model: Random Forest Regressor
n_estimators: 300
max_depth: 30
max_features: sqrt
n_jobs: -1 (parallel)
random_state: 42

Performance:
  - RÂ² Score: 0.4368
  - Explains ~44% of variance in paid amounts
  - Best performer among all tested models
```

### Feature Importance (Top 10)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEATURE IMPORTANCE - RANDOM FOREST                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. AMT_BILLED          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.45      â”‚
â”‚  2. AMT_BILLED_log      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         0.32      â”‚
â”‚  3. AMT_DEDUCT          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 0.08      â”‚
â”‚  4. AMT_COINS           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   0.06      â”‚
â”‚  5. Age                 â–ˆâ–ˆâ–ˆâ–ˆ                     0.03      â”‚
â”‚  6. CLIENT_LOS          â–ˆâ–ˆâ–ˆ                      0.02      â”‚
â”‚  7. FORM_TYPE_P         â–ˆâ–ˆ                       0.01      â”‚
â”‚  8. Gender_Code         â–ˆâ–ˆ                       0.01      â”‚
â”‚  9. PRODUCT_TYPE_PPO    â–ˆ                        0.01      â”‚
â”‚  10. ICD_Category_Z     â–ˆ                        0.01      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Insights

1. **Billed Amount is the strongest predictor** - The amount billed by providers explains ~45% of the paid amount
2. **Log transformation helps** - AMT_BILLED_log captures non-linear relationships
3. **Linear models underperform** - Low RÂ² (12-14%) indicates non-linear relationships in the data
4. **Ensemble methods excel** - Tree-based models capture complex feature interactions

---

## ğŸ“ Project Structure

```
predicting-Paid-amount-for-Claims-Data/
â”‚
â”œâ”€â”€ ğŸ“‚ config/                          # Configuration Management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.yaml                   # Central configuration file
â”‚
â”œâ”€â”€ ğŸ“‚ data/                            # Data Storage (gitignored)
â”‚   â”œâ”€â”€ raw/                            # Original immutable data
â”‚   â”œâ”€â”€ interim/                        # Intermediate processed data
â”‚   â”œâ”€â”€ processed/                      # Final analysis-ready data
â”‚   â””â”€â”€ external/                       # External reference data
â”‚
â”œâ”€â”€ ğŸ“‚ models/                          # Trained Models & Registry
â”‚   â””â”€â”€ registry.json                   # Model version registry
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                       # Jupyter Notebooks (Ordered)
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb         # ğŸ“¥ Data loading & validation
â”‚   â”œâ”€â”€ 02_exploratory_data_analysis.ipynb  # ğŸ“Š EDA & visualization
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb    # ğŸ”§ Feature transformation
â”‚   â”œâ”€â”€ 04_model_training.ipynb         # ğŸ¤– Model training & tuning
â”‚   â””â”€â”€ 05_model_evaluation.ipynb       # ğŸ“ˆ Evaluation & deployment
â”‚
â”œâ”€â”€ ğŸ“‚ src/                             # Source Code Package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                       # Configuration management
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ aws/                         # AWS Integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ s3_handler.py               # S3 operations
â”‚   â”‚   â”œâ”€â”€ glue_handler.py             # Glue ETL jobs
â”‚   â”‚   â””â”€â”€ redshift_handler.py         # Redshift data warehouse
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ data/                        # Data Processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py              # Chunked data loading
â”‚   â”‚   â”œâ”€â”€ data_processor.py           # Cleaning & transformation
â”‚   â”‚   â””â”€â”€ data_validator.py           # Data quality validation
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ features/                    # Feature Engineering
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py      # Feature creation & selection
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                      # Machine Learning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py            # Model training & tuning
â”‚   â”‚   â””â”€â”€ model_evaluator.py          # Metrics & visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ inference/                   # Production Inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ lambda_handler.py           # AWS Lambda handler
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                       # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                   # Logging configuration
â”‚       â””â”€â”€ helpers.py                  # Helper functions
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                           # Unit Tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â””â”€â”€ test_models.py
â”‚
â”œâ”€â”€ ğŸ“‚ reports/                         # Generated Reports
â”‚   â””â”€â”€ figures/                        # Visualization outputs
â”‚
â”œâ”€â”€ ğŸ“‚ PUBLICUSE_REF_TABLES/            # Reference Lookup Tables
â”‚   â”œâ”€â”€ REF_ICD_DIAG.txt                # ICD diagnosis codes
â”‚   â”œâ”€â”€ REF_CPT.txt                     # CPT procedure codes
â”‚   â””â”€â”€ ...                             # 17+ reference tables
â”‚
â”œâ”€â”€ .gitignore                          # Git ignore patterns
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # This file
```

---

## ğŸš€ Quick Start Guide

### Prerequisites

```bash
# Required
Python 3.10+
pip package manager

# Optional (for AWS features)
AWS CLI configured
AWS account with S3, Lambda, Glue access
```

### Installation

```bash
# 1. Clone repository
git clone https://github.com/yourusername/predicting-Paid-amount-for-Claims-Data.git
cd predicting-Paid-amount-for-Claims-Data

# 2. Create virtual environment
python -m venv venv

# Windows
.\venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
```

### Running the Pipeline

```bash
# Launch Jupyter and run notebooks in order
jupyter notebook notebooks/

# Or run via command line
jupyter nbconvert --execute notebooks/01_data_ingestion.ipynb
jupyter nbconvert --execute notebooks/02_exploratory_data_analysis.ipynb
jupyter nbconvert --execute notebooks/03_feature_engineering.ipynb
jupyter nbconvert --execute notebooks/04_model_training.ipynb
jupyter nbconvert --execute notebooks/05_model_evaluation.ipynb
```

### Demo Mode (No Raw Data Required)

All notebooks automatically create **demo data** if raw data files are not present:

```python
# Notebooks will output:
# "âš  Raw data file not found. Creating demo data for demonstration..."
# "âœ“ Created demo data: 50,000 rows"
```

---

## ğŸ”§ Pipeline Workflow

### Stage 1: Data Ingestion
```
Input:  PUBLICUSE_CLAIM_MC_2016.txt (3.7 GB, 17M rows)
Output: sampled_claims.parquet (1M unique claims)

Operations:
â”œâ”€â”€ Chunked reading (100K rows/chunk)
â”œâ”€â”€ Unique claim ID extraction
â”œâ”€â”€ Stratified sampling (1M claims)
â”œâ”€â”€ Reference table loading
â””â”€â”€ Data validation & profiling
```

### Stage 2: Exploratory Data Analysis
```
Input:  sampled_claims.parquet
Output: reports/figures/*.png

Analyses:
â”œâ”€â”€ Target distribution (AMT_PAID)
â”œâ”€â”€ Feature distributions
â”œâ”€â”€ Correlation analysis
â”œâ”€â”€ Missing value patterns
â””â”€â”€ Outlier detection
```

### Stage 3: Feature Engineering
```
Input:  sampled_claims.parquet
Output: processed_claims.parquet + transformer_state.pkl

Transformations:
â”œâ”€â”€ Gender encoding (Mâ†’1, Fâ†’0)
â”œâ”€â”€ Age encoding (90+â†’90, numeric)
â”œâ”€â”€ ICD code categorization (first letter)
â”œâ”€â”€ Dummy variable creation
â”œâ”€â”€ Z-score standardization
â””â”€â”€ Log transformations
```

### Stage 4: Model Training
```
Input:  processed_claims.parquet
Output: models/claims_predictor/

Models Trained:
â”œâ”€â”€ Lasso Regression (Î±=0.1)
â”œâ”€â”€ Ridge Regression (Î±=0.5)
â”œâ”€â”€ Random Forest (n=300, depth=30)
â””â”€â”€ Gradient Boosting (n=100, depth=5)
```

### Stage 5: Model Evaluation
```
Input:  Trained models + test data
Output: Evaluation metrics + visualizations

Outputs:
â”œâ”€â”€ RÂ², RMSE, MAE, MAPE metrics
â”œâ”€â”€ Actual vs Predicted plots
â”œâ”€â”€ Residual distributions
â”œâ”€â”€ Feature importance charts
â””â”€â”€ Production model registration
```

---

## â˜ï¸ AWS Integration

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AWS CLOUD INFRASTRUCTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚    S3       â”‚     â”‚   Glue      â”‚     â”‚  Redshift   â”‚                â”‚
â”‚  â”‚   Bucket    â”‚â”€â”€â”€â”€â–¶â”‚   ETL Job   â”‚â”€â”€â”€â”€â–¶â”‚   Cluster   â”‚                â”‚
â”‚  â”‚             â”‚     â”‚             â”‚     â”‚             â”‚                â”‚
â”‚  â”‚ â€¢ Raw Data  â”‚     â”‚ â€¢ Transform â”‚     â”‚ â€¢ Analytics â”‚                â”‚
â”‚  â”‚ â€¢ Processed â”‚     â”‚ â€¢ Catalog   â”‚     â”‚ â€¢ Queries   â”‚                â”‚
â”‚  â”‚ â€¢ Models    â”‚     â”‚ â€¢ Schedule  â”‚     â”‚ â€¢ Reports   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                                                                â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Lambda    â”‚â—€â”€â”€â”€â”€â”‚ API Gateway â”‚â—€â”€â”€â”€â”€â”‚   Client    â”‚                â”‚
â”‚  â”‚  Function   â”‚     â”‚    REST     â”‚     â”‚  Applicationâ”‚                â”‚
â”‚  â”‚             â”‚     â”‚             â”‚     â”‚             â”‚                â”‚
â”‚  â”‚ â€¢ Load Modelâ”‚     â”‚ â€¢ /predict  â”‚     â”‚ â€¢ Web App   â”‚                â”‚
â”‚  â”‚ â€¢ Inference â”‚     â”‚ â€¢ Auth      â”‚     â”‚ â€¢ Mobile    â”‚                â”‚
â”‚  â”‚ â€¢ Response  â”‚     â”‚ â€¢ Throttle  â”‚     â”‚ â€¢ API       â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### S3 Usage

```python
from src.aws.s3_handler import S3Handler

s3 = S3Handler(bucket_name="medical-claims-ml", region="us-east-1")

# Upload processed data
s3.upload_dataframe(df, "processed/claims.parquet")

# Upload trained model
s3.upload_model(model, "models/v1.0/model.pkl")

# Download for inference
model = s3.download_model("models/v1.0/model.pkl")
```

### Lambda Deployment

```python
# Environment Variables
MODEL_S3_BUCKET=medical-claims-ml
MODEL_S3_KEY=models/v1.0/model.pkl

# Invoke
POST /predict
{
  "amt_billed": 1500.00,
  "amt_deduct": 100.00,
  "age": 45,
  "form_type": "P"
}

# Response
{
  "success": true,
  "predictions": {
    "predicted_amount": 750.50,
    "confidence_interval": {"lower": 638.42, "upper": 862.58}
  }
}
```

---

## ğŸ“¡ API Reference

### Prediction Endpoint

**POST** `/predict`

#### Request Schema

```json
{
  "amt_billed": 1500.00,      // Required: Billed amount ($)
  "amt_deduct": 100.00,       // Optional: Deductible amount ($)
  "amt_coins": 50.00,         // Optional: Coinsurance amount ($)
  "age": 45,                  // Optional: Patient age (default: 45)
  "gender_code": 1,           // Optional: 1=Male, 0=Female
  "client_los": 0,            // Optional: Length of stay (days)
  "form_type": "P",           // Optional: P=Professional, I=Institutional
  "sv_stat": "P",             // Optional: Service status
  "product_type": "PPO",      // Optional: HMO, PPO, POS
  "icd_category": "Z"         // Optional: ICD diagnosis category
}
```

#### Response Schema

```json
{
  "success": true,
  "request_id": "abc-123-def",
  "predictions": {
    "predicted_amount": 750.50,
    "confidence_interval": {
      "lower": 638.42,
      "upper": 862.58
    },
    "model_version": "1.0.0"
  }
}
```

#### Batch Prediction

```json
// Request
[
  {"amt_billed": 1500.00, "age": 45},
  {"amt_billed": 2500.00, "age": 65}
]

// Response
{
  "success": true,
  "predictions": [
    {"predicted_amount": 750.50, ...},
    {"predicted_amount": 1250.75, ...}
  ]
}
```

---

## âš™ï¸ Configuration

### Main Configuration (`config/settings.yaml`)

```yaml
# Project Information
project:
  name: "Medical Claims Paid Amount Prediction"
  version: "1.0.0"

# Data Configuration
data:
  raw_data_file: "PUBLICUSE_CLAIM_MC_2016.txt"
  delimiter: "|"
  total_rows: 16982295
  chunk_size: 100000
  sample_size: 1000000
  target_column: "AMT_PAID"

# Model Configuration
model:
  test_size: 0.2
  random_state: 42
  
  random_forest:
    n_estimators: 300
    max_depth: 30
    max_features: "sqrt"

# AWS Configuration
aws:
  region: "us-east-1"
  s3:
    bucket_name: "medical-claims-ml-pipeline"
    raw_data_prefix: "raw/"
    processed_data_prefix: "processed/"
    models_prefix: "models/"
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html

# Run specific test file
pytest tests/test_models.py -v
```

---

## ğŸ“ˆ Future Improvements

1. **Deep Learning Models**: Implement neural networks for complex patterns
2. **AutoML Integration**: Add automated model selection (AutoML)
3. **Real-time Inference**: Stream processing with Kinesis
4. **Model Monitoring**: Drift detection and retraining triggers
5. **Feature Store**: Centralized feature management
6. **A/B Testing**: Model comparison in production

---

## ğŸ‘¥ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™ Acknowledgments

- New Hampshire Insurance Department for public claims data
- scikit-learn, pandas, and numpy communities
- AWS for cloud infrastructure

---

<p align="center">
  <strong>Built with â¤ï¸ for Healthcare Analytics</strong>
  <br><br>
  <img src="https://img.shields.io/badge/Made%20with-Python-1f425f.svg" />
  <img src="https://img.shields.io/badge/ML-Production%20Ready-success.svg" />
</p>
