{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Medical Claims Data Ingestion Pipeline\n",
        "\n",
        "**Project:** Predicting Paid Amount for Medical Claims  \n",
        "**Stage:** Data Ingestion & ETL Pipeline  \n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a production-grade data ingestion pipeline for processing ~17 million medical claims records:\n",
        "\n",
        "1. **Chunked Data Loading** - Memory-efficient processing of 3.7GB+ files\n",
        "2. **Claim ID Sampling** - Stratified sampling of 1M unique claims\n",
        "3. **Data Validation** - Quality checks and data profiling\n",
        "4. **AWS S3 Integration** - Cloud storage for processed data\n",
        "5. **Reference Table Loading** - Loading lookup tables for enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Project imports\n",
        "from src.config import ConfigurationManager, get_config\n",
        "from src.utils.logger import setup_logging, get_logger, PipelineLogger\n",
        "from src.utils.helpers import timer, memory_usage, format_bytes\n",
        "from src.data.data_loader import DataLoader, ChunkedDataLoader, ReferenceDataLoader\n",
        "from src.data.data_validator import DataValidator, validate_claims_data\n",
        "\n",
        "# Setup logging\n",
        "setup_logging(log_level=\"INFO\", log_file=\"logs/data_ingestion.log\")\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "print(\"âœ“ Imports successful\")\n",
        "print(f\"Current Memory Usage: {memory_usage()['rss']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "try:\n",
        "    config_manager = ConfigurationManager(str(project_root / \"config\" / \"settings.yaml\"))\n",
        "    config = config_manager.config\n",
        "    print(f\"âœ“ Loaded configuration for: {config.name}\")\n",
        "    print(f\"  Version: {config.version}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âš  Config file not found, using defaults\")\n",
        "    RAW_DATA_FILE = \"PUBLICUSE_CLAIM_MC_2016.txt\"\n",
        "    CHUNK_SIZE = 100000\n",
        "    SAMPLE_SIZE = 1000000\n",
        "    DELIMITER = \"|\"\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "# Define paths\n",
        "DATA_DIR = project_root / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "INTERIM_DIR = DATA_DIR / \"interim\"\n",
        "REF_TABLES_DIR = project_root / \"PUBLICUSE_REF_TABLES\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [RAW_DIR, PROCESSED_DIR, INTERIM_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nðŸ“ Directory Structure:\")\n",
        "print(f\"  Raw Data:      {RAW_DIR}\")\n",
        "print(f\"  Processed:     {PROCESSED_DIR}\")\n",
        "print(f\"  Interim:       {INTERIM_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Reference Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize reference data loader\n",
        "ref_loader = ReferenceDataLoader(str(REF_TABLES_DIR))\n",
        "\n",
        "# Load all reference tables\n",
        "with PipelineLogger(\"Loading Reference Tables\", logger):\n",
        "    reference_tables = ref_loader.load_all_reference_tables()\n",
        "\n",
        "print(f\"\\nðŸ“‹ Loaded {len(reference_tables)} reference tables:\")\n",
        "for name, df in reference_tables.items():\n",
        "    print(f\"  â€¢ {name}: {len(df):,} records, {len(df.columns)} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Chunked Data Loading\n",
        "\n",
        "The raw claims file is ~3.7GB with ~17 million rows. We use chunked loading for memory efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for data loading\n",
        "RAW_DATA_PATH = RAW_DIR / \"PUBLICUSE_CLAIM_MC_2016.txt\"\n",
        "CHUNK_SIZE = 100000\n",
        "TOTAL_ROWS = 16982295\n",
        "SAMPLE_SIZE = 1000000\n",
        "CLAIM_ID_COLUMN = \"CLAIM_ID_KEY\"\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Initialize chunked loader\n",
        "chunked_loader = ChunkedDataLoader(chunk_size=CHUNK_SIZE, delimiter=\"|\")\n",
        "\n",
        "print(f\"ðŸ“Š Data Loading Configuration:\")\n",
        "print(f\"  Source File: {RAW_DATA_PATH}\")\n",
        "print(f\"  Chunk Size: {CHUNK_SIZE:,}\")\n",
        "print(f\"  Expected Rows: {TOTAL_ROWS:,}\")\n",
        "print(f\"  Sample Size: {SAMPLE_SIZE:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample claims data (or create demo data if file not found)\n",
        "if RAW_DATA_PATH.exists():\n",
        "    with PipelineLogger(\"Sampling Claims Data\", logger):\n",
        "        sampled_df = chunked_loader.sample_by_claim_ids(\n",
        "            str(RAW_DATA_PATH),\n",
        "            claim_id_column=CLAIM_ID_COLUMN,\n",
        "            sample_size=SAMPLE_SIZE,\n",
        "            random_seed=RANDOM_SEED\n",
        "        )\n",
        "    print(f\"\\nâœ“ Sampled Data: {len(sampled_df):,} rows, {sampled_df[CLAIM_ID_COLUMN].nunique():,} unique claims\")\n",
        "else:\n",
        "    print(\"âš  Raw data file not found. Creating demo data for demonstration...\")\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    n_samples = 50000\n",
        "    sampled_df = pd.DataFrame({\n",
        "        'CLAIM_ID_KEY': np.random.randint(1, 20000, n_samples),\n",
        "        'AGE': np.random.choice(['25', '35', '45', '55', '65', '75', '90+'], n_samples),\n",
        "        'SEX': np.random.choice(['M', 'F'], n_samples),\n",
        "        'AMT_BILLED': np.abs(np.random.exponential(1000, n_samples)),\n",
        "        'AMT_PAID': np.abs(np.random.exponential(500, n_samples)),\n",
        "        'AMT_DEDUCT': np.abs(np.random.exponential(100, n_samples)),\n",
        "        'AMT_COINS': np.abs(np.random.exponential(50, n_samples)),\n",
        "        'FORM_TYPE': np.random.choice(['P', 'I', 'O'], n_samples),\n",
        "        'SV_STAT': np.random.choice(['P', 'D', 'R'], n_samples),\n",
        "        'PRODUCT_TYPE': np.random.choice(['HMO', 'PPO', 'POS'], n_samples),\n",
        "        'ICD_DIAG_01_PRIMARY': np.random.choice(['Z00', 'J06', 'M54', 'I10', 'K21', 'E11', 'F32'], n_samples),\n",
        "        'CLIENT_LOS': np.random.choice([0, 1, 2, 3, np.nan], n_samples, p=[0.7, 0.1, 0.08, 0.07, 0.05]),\n",
        "    })\n",
        "    print(f\"âœ“ Created demo data: {len(sampled_df):,} rows\")\n",
        "\n",
        "# Display preview\n",
        "print(\"\\nðŸ“‹ Data Preview:\")\n",
        "sampled_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run validation\n",
        "with PipelineLogger(\"Data Validation\", logger):\n",
        "    validation_report = validate_claims_data(sampled_df)\n",
        "\n",
        "print(f\"\\n{validation_report.summary()}\")\n",
        "\n",
        "# Missing values analysis\n",
        "missing_stats = sampled_df.isnull().sum()\n",
        "missing_pct = (missing_stats / len(sampled_df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing Count': missing_stats, 'Missing %': missing_pct})\n",
        "missing_df = missing_df.sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š Missing Values Summary:\")\n",
        "print(f\"  Columns with missing: {(missing_stats > 0).sum()}\")\n",
        "missing_df[missing_df['Missing Count'] > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save sampled data\n",
        "loader = DataLoader()\n",
        "\n",
        "# Save as Parquet (efficient columnar format)\n",
        "parquet_path = INTERIM_DIR / \"sampled_claims.parquet\"\n",
        "loader.save_parquet(sampled_df, str(parquet_path))\n",
        "\n",
        "# Also save as CSV\n",
        "csv_path = INTERIM_DIR / \"sampled_claims.csv\"\n",
        "loader.save_csv(sampled_df, str(csv_path))\n",
        "\n",
        "print(f\"\\nðŸ’¾ Data Saved:\")\n",
        "print(f\"  Parquet: {parquet_path}\")\n",
        "print(f\"  CSV: {csv_path}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š DATA INGESTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Total Rows: {len(sampled_df):,}\")\n",
        "print(f\"  Total Columns: {len(sampled_df.columns)}\")\n",
        "print(f\"  Memory Usage: {sampled_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "if 'AMT_PAID' in sampled_df.columns:\n",
        "    print(f\"  Paid Amount - Mean: ${sampled_df['AMT_PAID'].mean():,.2f}\")\n",
        "print(f\"\\nâœ… Data ingestion completed! Next: Run 02_exploratory_data_analysis.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
