{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”§ Feature Engineering Pipeline\n",
        "\n",
        "**Project:** Predicting Paid Amount for Medical Claims  \n",
        "**Stage:** Feature Engineering & Data Preparation  \n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Data Cleaning** - Handle missing values and remove invalid records\n",
        "2. **Feature Transformation** - Encode categorical variables, normalize numerics\n",
        "3. **Feature Creation** - Create derived features (diagnosis counts, ICD categories)\n",
        "4. **Feature Selection** - Select most predictive features\n",
        "5. **Data Preparation** - Prepare final dataset for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from src.utils.logger import setup_logging, get_logger, PipelineLogger\n",
        "from src.data.data_loader import DataLoader\n",
        "from src.data.data_processor import DataCleaner, DataProcessor, DataPipeline\n",
        "from src.features.feature_engineering import FeatureEngineer, FeatureSelector\n",
        "\n",
        "setup_logging(log_level=\"INFO\")\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Paths\n",
        "INTERIM_DIR = project_root / \"data\" / \"interim\"\n",
        "PROCESSED_DIR = project_root / \"data\" / \"processed\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ“ Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "parquet_path = INTERIM_DIR / \"sampled_claims.parquet\"\n",
        "\n",
        "if parquet_path.exists():\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "else:\n",
        "    # Create demo data\n",
        "    np.random.seed(42)\n",
        "    n = 50000\n",
        "    df = pd.DataFrame({\n",
        "        'CLAIM_ID_KEY': np.random.randint(1, 20000, n),\n",
        "        'AGE': np.random.choice(['25', '35', '45', '55', '65', '75', '90+'], n),\n",
        "        'SEX': np.random.choice(['M', 'F'], n),\n",
        "        'AMT_BILLED': np.abs(np.random.exponential(1000, n)),\n",
        "        'AMT_PAID': np.abs(np.random.exponential(500, n)),\n",
        "        'AMT_DEDUCT': np.abs(np.random.exponential(100, n)),\n",
        "        'AMT_COINS': np.abs(np.random.exponential(50, n)),\n",
        "        'FORM_TYPE': np.random.choice(['P', 'I', 'O'], n),\n",
        "        'SV_STAT': np.random.choice(['P', 'D', 'R'], n),\n",
        "        'PRODUCT_TYPE': np.random.choice(['HMO', 'PPO', 'POS'], n),\n",
        "        'ICD_DIAG_01_PRIMARY': np.random.choice(['Z00', 'J06', 'M54', 'I10', 'K21'], n),\n",
        "        'CLIENT_LOS': np.random.choice([0, 1, 2, 3, np.nan], n, p=[0.7, 0.1, 0.08, 0.07, 0.05]),\n",
        "    })\n",
        "\n",
        "print(f\"âœ“ Loaded data: {len(df):,} rows, {len(df.columns)} columns\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize cleaners and processors\n",
        "cleaner = DataCleaner(missing_threshold=1000000)\n",
        "processor = DataProcessor()\n",
        "\n",
        "with PipelineLogger(\"Data Cleaning\", logger):\n",
        "    initial_shape = df.shape\n",
        "    \n",
        "    # Handle missing values\n",
        "    df = cleaner.handle_missing_values(df, fill_values={'CLIENT_LOS': 0})\n",
        "    \n",
        "    # Remove negative amounts\n",
        "    amount_cols = ['AMT_BILLED', 'AMT_PAID', 'AMT_DEDUCT', 'AMT_COINS']\n",
        "    amount_cols = [c for c in amount_cols if c in df.columns]\n",
        "    df = cleaner.remove_negative_values(df, amount_cols)\n",
        "    \n",
        "    # Drop remaining rows with missing values\n",
        "    df = df.dropna()\n",
        "    \n",
        "    print(f\"Shape: {initial_shape} -> {df.shape}\")\n",
        "    print(f\"Removed {initial_shape[0] - len(df):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with PipelineLogger(\"Feature Transformation\", logger):\n",
        "    # Encode gender\n",
        "    if 'SEX' in df.columns:\n",
        "        df = processor.encode_gender(df, column='SEX', new_column='Gender_Code')\n",
        "    \n",
        "    # Encode age\n",
        "    if 'AGE' in df.columns:\n",
        "        df = processor.encode_age(df, column='AGE', new_column='Age')\n",
        "    \n",
        "    # Extract ICD category\n",
        "    if 'ICD_DIAG_01_PRIMARY' in df.columns:\n",
        "        df = processor.extract_code_category(df, 'ICD_DIAG_01_PRIMARY', 'ICD_Category')\n",
        "\n",
        "print(f\"âœ“ Transformed data: {len(df.columns)} columns\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize feature engineer\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "with PipelineLogger(\"Creating Features\", logger):\n",
        "    # Remove ID column if present\n",
        "    if 'CLAIM_ID_KEY' in df.columns:\n",
        "        df = df.drop(columns=['CLAIM_ID_KEY'])\n",
        "    \n",
        "    # Define columns\n",
        "    categorical_cols = ['FORM_TYPE', 'SV_STAT', 'PRODUCT_TYPE', 'ICD_Category']\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "    \n",
        "    numerical_cols = ['AMT_BILLED', 'AMT_DEDUCT', 'AMT_COINS', 'CLIENT_LOS', 'Age', 'Gender_Code']\n",
        "    numerical_cols = [c for c in numerical_cols if c in df.columns]\n",
        "    \n",
        "    # Create dummy variables\n",
        "    df = feature_engineer.create_dummy_variables(df, categorical_cols)\n",
        "    \n",
        "    # Create log features for amounts\n",
        "    if 'AMT_BILLED' in df.columns:\n",
        "        df['AMT_BILLED_log'] = np.log1p(df['AMT_BILLED'])\n",
        "\n",
        "print(f\"âœ“ Final features: {len(df.columns)} columns\")\n",
        "print(f\"Columns: {list(df.columns)[:15]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "TARGET_COLUMN = 'AMT_PAID'\n",
        "\n",
        "if TARGET_COLUMN in df.columns:\n",
        "    y = df[TARGET_COLUMN]\n",
        "    X = df.drop(columns=[TARGET_COLUMN])\n",
        "    \n",
        "    # Standardize numerical features (excluding target)\n",
        "    num_cols_to_scale = [c for c in X.columns if X[c].dtype in ['float64', 'int64']]\n",
        "    X = feature_engineer.fit_scalers(X, num_cols_to_scale, method='zscore')\n",
        "    \n",
        "    print(f\"âœ“ Features (X): {X.shape}\")\n",
        "    print(f\"âœ“ Target (y): {y.shape}\")\n",
        "    print(f\"  Target mean: ${y.mean():,.2f}\")\n",
        "    print(f\"  Target std: ${y.std():,.2f}\")\n",
        "else:\n",
        "    print(\"âš  Target column not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data\n",
        "processed_df = pd.concat([X, y], axis=1)\n",
        "output_path = PROCESSED_DIR / \"processed_claims.parquet\"\n",
        "processed_df.to_parquet(output_path, index=False)\n",
        "\n",
        "# Save transformer state\n",
        "feature_engineer.save_state(str(PROCESSED_DIR / \"transformer_state.pkl\"))\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saved:\")\n",
        "print(f\"  Processed data: {output_path}\")\n",
        "print(f\"  Transformer state: {PROCESSED_DIR / 'transformer_state.pkl'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Final shape: {processed_df.shape}\")\n",
        "print(f\"  Feature columns: {len(X.columns)}\")\n",
        "print(f\"\\nâœ… Feature engineering completed! Next: Run 04_model_training.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
