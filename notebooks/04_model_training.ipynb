{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Model Training Pipeline\n",
        "\n",
        "**Project:** Predicting Paid Amount for Medical Claims  \n",
        "**Stage:** Model Training & Selection  \n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Data Preparation** - Load processed features, train/test split\n",
        "2. **Linear Models** - Lasso, Ridge regression\n",
        "3. **Ensemble Models** - Random Forest, Gradient Boosting\n",
        "4. **Model Comparison** - Compare all models\n",
        "5. **Model Selection** - Save best model for deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from src.utils.logger import setup_logging, get_logger, PipelineLogger\n",
        "from src.models.model_trainer import ModelTrainer, ModelRegistry\n",
        "from src.models.model_evaluator import ModelEvaluator\n",
        "\n",
        "setup_logging(log_level=\"INFO\")\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Paths\n",
        "PROCESSED_DIR = project_root / \"data\" / \"processed\"\n",
        "MODELS_DIR = project_root / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úì Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "parquet_path = PROCESSED_DIR / \"processed_claims.parquet\"\n",
        "TARGET = 'AMT_PAID'\n",
        "\n",
        "if parquet_path.exists():\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "else:\n",
        "    # Create demo data\n",
        "    np.random.seed(42)\n",
        "    n = 50000\n",
        "    n_features = 20\n",
        "    X_demo = pd.DataFrame(np.random.randn(n, n_features), \n",
        "                          columns=[f'feature_{i}' for i in range(n_features)])\n",
        "    y_demo = 500 + 200 * X_demo['feature_0'] + 100 * X_demo['feature_1'] + np.random.randn(n) * 100\n",
        "    df = pd.concat([X_demo, pd.Series(y_demo, name=TARGET)], axis=1)\n",
        "\n",
        "# Separate features and target\n",
        "y = df[TARGET]\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "print(f\"‚úì Data loaded: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "\n",
        "# Train/test split\n",
        "trainer = ModelTrainer(random_state=42, test_size=0.2)\n",
        "X_train, X_test, y_train, y_test = trainer.split_data(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Linear Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with PipelineLogger(\"Training Linear Models\", logger):\n",
        "    # Lasso\n",
        "    lasso_result = trainer.train_model('lasso', X_train, y_train, X_test, y_test,\n",
        "                                       params={'alpha': 0.1})\n",
        "    \n",
        "    # Ridge  \n",
        "    ridge_result = trainer.train_model('ridge', X_train, y_train, X_test, y_test,\n",
        "                                       params={'alpha': 0.5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Ensemble Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with PipelineLogger(\"Training Ensemble Models\", logger):\n",
        "    # Random Forest\n",
        "    rf_result = trainer.train_model('random_forest', X_train, y_train, X_test, y_test,\n",
        "                                    params={'n_estimators': 100, 'max_depth': 20, 'n_jobs': -1})\n",
        "    \n",
        "    # Gradient Boosting\n",
        "    gb_result = trainer.train_model('gradient_boosting', X_train, y_train, X_test, y_test,\n",
        "                                    params={'n_estimators': 100, 'max_depth': 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "for name, result in trainer.trained_models.items():\n",
        "    metrics = result.metadata.validation_metrics\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'R¬≤': metrics['r2'],\n",
        "        'RMSE': metrics['rmse'],\n",
        "        'MAE': metrics['mae']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(results).sort_values('R¬≤', ascending=False)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Get best model\n",
        "best_result = trainer.get_best_model()\n",
        "print(f\"\\nüèÜ Best Model: {best_result.metadata.model_type}\")\n",
        "print(f\"   Validation R¬≤: {best_result.metadata.validation_metrics['r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model registry\n",
        "registry = ModelRegistry(str(MODELS_DIR))\n",
        "\n",
        "# Save best model\n",
        "best_result = trainer.get_best_model()\n",
        "model_path = registry.save_model(best_result, model_name=\"claims_predictor\")\n",
        "\n",
        "# Set as production model\n",
        "registry.set_production_model(\"claims_predictor\")\n",
        "\n",
        "print(f\"\\nüíæ Model saved to: {model_path}\")\n",
        "print(f\"\\n‚úÖ Training completed! Next: Run 05_model_evaluation.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
