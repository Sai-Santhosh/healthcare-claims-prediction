{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Model Evaluation & Analysis\n",
        "\n",
        "**Project:** Predicting Paid Amount for Medical Claims  \n",
        "**Stage:** Model Evaluation & Deployment Readiness  \n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Load Production Model** - Load the best trained model\n",
        "2. **Performance Metrics** - Calculate comprehensive metrics\n",
        "3. **Visualization** - Prediction plots, residual analysis\n",
        "4. **Feature Importance** - Understand model decisions\n",
        "5. **Business Impact** - Translate metrics to business value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.utils.logger import setup_logging, get_logger\n",
        "from src.models.model_trainer import ModelRegistry\n",
        "from src.models.model_evaluator import ModelEvaluator\n",
        "\n",
        "setup_logging(log_level=\"INFO\")\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Paths\n",
        "PROCESSED_DIR = project_root / \"data\" / \"processed\"\n",
        "MODELS_DIR = project_root / \"models\"\n",
        "FIGURES_DIR = project_root / \"reports\" / \"figures\"\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ“ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load production model\n",
        "registry = ModelRegistry(str(MODELS_DIR))\n",
        "TARGET = 'AMT_PAID'\n",
        "\n",
        "try:\n",
        "    model, metadata = registry.get_production_model()\n",
        "    print(f\"âœ“ Loaded model: {metadata.model_name}\")\n",
        "    print(f\"  Type: {metadata.model_type}\")\n",
        "    print(f\"  Version: {metadata.version}\")\n",
        "except:\n",
        "    print(\"âš  No production model found. Creating demo model...\")\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "    metadata = None\n",
        "\n",
        "# Load test data\n",
        "parquet_path = PROCESSED_DIR / \"processed_claims.parquet\"\n",
        "if parquet_path.exists():\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "else:\n",
        "    np.random.seed(42)\n",
        "    n = 10000\n",
        "    n_features = 20\n",
        "    X_demo = pd.DataFrame(np.random.randn(n, n_features), \n",
        "                          columns=[f'feature_{i}' for i in range(n_features)])\n",
        "    y_demo = 500 + 200 * X_demo['feature_0'] + 100 * X_demo['feature_1'] + np.random.randn(n) * 100\n",
        "    df = pd.concat([X_demo, pd.Series(y_demo, name=TARGET)], axis=1)\n",
        "\n",
        "y = df[TARGET]\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# Create test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit demo model if needed\n",
        "if metadata is None:\n",
        "    model.fit(X.iloc[:8000], y.iloc[:8000])\n",
        "    \n",
        "print(f\"\\nâœ“ Test set: {len(X_test):,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(figures_dir=str(FIGURES_DIR))\n",
        "\n",
        "# Evaluate model\n",
        "metrics, y_pred = evaluator.evaluate_model(model, X_test, y_test, \"Production Model\")\n",
        "\n",
        "print(\"\\nðŸ“Š Performance Metrics:\")\n",
        "for metric, value in metrics.to_dict().items():\n",
        "    print(f\"  {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction plots\n",
        "evaluator.plot_predictions(y_test.values, y_pred, \"Production Model\")\n",
        "evaluator.plot_residual_distribution(y_test.values, y_pred, \"Production Model\")\n",
        "\n",
        "# Feature importance\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = dict(zip(X_test.columns, model.feature_importances_))\n",
        "    evaluator.plot_feature_importance(importances, top_n=15, model_name=\"Production Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š MODEL EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  RÂ² Score: {metrics.r2:.4f}\")\n",
        "print(f\"  RMSE: ${metrics.rmse:,.2f}\")\n",
        "print(f\"  MAE: ${metrics.mae:,.2f}\")\n",
        "print(f\"  MAPE: {metrics.mape:.2%}\")\n",
        "\n",
        "print(f\"\\nBusiness Impact:\")\n",
        "avg_actual = y_test.mean()\n",
        "print(f\"  Average Claim: ${avg_actual:,.2f}\")\n",
        "print(f\"  Average Error: ${metrics.mae:,.2f} ({metrics.mae/avg_actual*100:.1f}% of avg claim)\")\n",
        "\n",
        "print(f\"\\nModel is ready for deployment!\")\n",
        "print(f\"  Model saved in: {MODELS_DIR}\")\n",
        "print(f\"  Figures saved in: {FIGURES_DIR}\")\n",
        "print(f\"\\nâœ… Evaluation completed!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
